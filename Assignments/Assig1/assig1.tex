\documentclass[12pt]{article}
\usepackage{fullpage,enumitem,amsmath,amssymb,graphicx,grffile,float,listings}



\begin{document}
    %Title Section
    \begin{flushleft}
    \LARGE CS229 Fall 2017\\
    \LARGE Problem Set \#1: Supervised Learning\\
    \textbf{\normalsize Author: LFhase \quad rimemosa@163.com}
    \end{flushleft} 
    \noindent
    \rule{\linewidth}{0.4pt}
    %Title Section

    %Problem and Solution

    \section*{Logistic regression  }

    \begin{enumerate}[label=(\alph*)]

    \item 
    {
        Given that 
        $$J(\theta)= \frac{1}{m} \sum_{i=1}^{m}log(1+e^{-y^{(i)}\theta^{T}x^{(i)}})$$\\
        we can get
        $$\frac{\partial J(\theta)}{\partial \theta_{i}}
            =  \frac{1}{m} \sum_{k=1}^{m} \frac{-y^{(k)}x^{(k)}_i}{1+e^{y^{(k)}\theta^{T}x^{(k)}}}
        $$\\
        then
        $$\frac{\partial J(\theta)}{\partial \theta_{i} \partial \theta_{j}}
            =  \frac{1}{m} \sum_{k=1}^{m} 
            \frac{x^{(k)}_ix^{(k)}_je^{y^{(k)}\theta^{T}x^{(k)}}}
            {(1+e^{y^{(k)}\theta^{T}x^{(k)}})^2}
        $$\\
        which is $ H_{ij} $\\
        so
        \begin{equation*} 
        \begin{split}
            Z^THZ &= \sum_{i=1}^n \sum_{j=1}^n z_iH_{ij}z_j \\
            &= \frac{1}{m} \sum_{k=1}^{m} 
            \frac{\sum_{i=1}^n \sum_{j=1}^n z_ix^{(k)}_ix^{(k)}_jz_je^{y^{(k)}\theta^{T}x^{(k)}}}{(1+e^{y^{(k)}\theta^{T}x^{(k)}})^2}
            \\
        \end{split}
        \end{equation*} 
        known that 
        $$ \sum_{i=1}^n \sum_{j=1}^n z_ix^{(k)}_ix^{(k)}_jz_j = (X^TZ)^2 \geq 0 $$
        $$ \frac{e^{y^{(k)}\theta^{T}x^{(k)}}}{(1+e^{y^{(k)}\theta^{T}x^{(k)}})^2} > 0 $$
        we can easily get $$ Z^THZ \geq 0 $$
    }
    \newpage
    \item 
    {
        Firstly, we plot the original data

        \begin{figure}[H]
            \centering
            \includegraphics[width=0.80\textwidth]{figure/LogisticRegression/original_data.png}
        \end{figure}

        To implement Newtonâ€™s Method, we calculate the value of 
        $$\frac{1}{m} \sum_{k=1}^{m} \frac{-y^{(k)}x^{(k)}_i}{1+e^{y^{(k)}\theta^{T}x^{(k)}}}
        $$
        and $\textbf{Hessian}$\\
        then using the \textbf{update rule}
        $$ \theta := \theta - H^{-1}\bigtriangledown_\theta l(\theta) $$
    }
    
    \newpage
    \item 
    {
        Through 6 iterations, we finally get the result
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.80\textwidth]{figure/LogisticRegression/Iteration6.png}
        \end{figure}
        and the $ \theta $ is  [-2.6205116,   0.76037154,  1.17194674].
    }

    \end{enumerate}

    \newpage
    \section*{ Poisson regression and the exponential family}

    \begin{enumerate}[label=(\alph*)]

    \item 
    Firstly, we get 
    \begin{equation*} 
        \begin{split}
            p(y;\lambda) &= \frac{e^{-\lambda}\lambda^{y}}{y!} \\
            &= \frac{1}{y!}exp(log\lambda^y - loge^\lambda) \\
            &= \frac{1}{y!}exp(log\lambda y - \lambda) \\
    \end{split}
    \end{equation*} 
    It's easy to know that
    $$ log\lambda = \eta, \lambda = e^\eta $$
    $$ T(y) = y $$
    $$ a(\eta) = -e^\eta $$
    $$ b(y) = \frac{1}{y!} $$
    and Poisson distribution is in the exponential family.

    \item 
    Known that if $ Y \sim P(\lambda) $, then $ E[Y] = \lambda $ ,
    \begin{equation*} 
        \begin{split}
            h_\theta(x) &= E[y|x;\theta] \\
                        &= \lambda \\
                        &= e^\eta = e^{\theta^Tx} \\
        \end{split}
    \end{equation*} 
    so the response function for the family is 
    $$ g(\eta) = e^\eta $$.

    \item 
    Firstly, we can get likelihood function $L(\theta)$
    \begin{equation*} 
        \begin{split}
            L(\theta) &= \Pi_{k=1}^m P(y^{(k)}|x^{(k)};\lambda) \\
                      &= \Pi_{k=1}^m \frac{e^{-\lambda}\lambda^{y^{(k)}}}{y^{(k)}!} \\
        \end{split}
    \end{equation*} 
    Then, the log-likelihood function $l(\theta)$
    \begin{equation*} 
        \begin{split}
            l(\theta) 
            &= \sum_{k=1}^m (-logy^{(k)}!) + (-\lambda) + y^{(k)}log\lambda \\
            &= \sum_{k=1}^m (-logy^{(k)}!) + (-e^{\theta^Tx^{(k)}}) + y^{(k)}\theta^Tx^{(k)} \\
        \end{split}
    \end{equation*} 

    Then, we can get the derivative of the log-likelihood with respect to $\theta_i$
    $$ \frac{\partial l(\theta)}{\partial \theta_i} 
       = \sum_{k=1}^m (y^{(k)}-e^{\theta^Tx^{(k)}})x^{(k)}_i
    $$
    So, the stochastic gradient ascent learning rule is
    $$ \theta_i = \theta_i + \alpha(y^{(k)}-e^{\theta^Tx^{(k)}})x^{(k)}_i
    $$

    \item 
    
    Firstly, we can know that
    $$ p(y|x;\theta) = b(y)exp(\eta^Ty-a(\eta)) $$
    then, for a simple training data $(x,y)$ in stochastic gradient ascent,
    $$ \frac{\partial p(y|x;\theta)}{\partial \theta_i} 
        = x_i(y-\frac{\partial a(\eta)}{\partial \eta})
    $$
    known that,
    $$ \int_{y}p(y|x;\eta) = 1 $$
    we can imply derivation in both sides, and get
    $$ \int_{y}p(y|x;\eta)(y-\frac{\partial a(\eta)}{\partial \eta}) = 0 $$
    finally, we get
    $$ \frac{\partial a(\eta)}{\partial \eta} = \int_{y}p(y|x;\eta)y = h_\theta(x) $$
    so the gradient we get is
    $$ \frac{\partial p(y|x;\theta)}{\partial \theta_i} 
        = x_i(y-h_\theta(x))
    $$
    the update rule is
    $$ \theta_i = \theta_i - \alpha(h_\theta(x)-y)x_i $$
    \end{enumerate}

    

    \newpage
    \section*{Gaussian discriminant analysis   }
    \begin{enumerate}[label=(\alph*)]
    
    \item 

    gg

    \end{enumerate}


\end{document}